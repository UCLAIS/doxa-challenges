{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCLAIS Tutorial Series Challenge 3\n",
    "\n",
    "We are proud to present you with our third challenge of the 2022-23 UCLAIS tutorial series: Sentiment Analysis on the Climate Change problem. You will be introduced to another super exciting domain in Machine Learning, which is Natural Language Processing ðŸ™€.\n",
    "\n",
    "This Jupyter notebook will guide you through the various general stages involved in end-to-end NLP projects, including data visualisation, data preprocessing, model selection, model training, and model evaluation. Finally, you will get the chance to submit your results to [DOXA](https://doxaai.com/).\n",
    "\n",
    "If you do not already have a DOXA account, please [sign up](https://doxaai.com/sign-up) first before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background & Motivation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Background**: \n",
    "\n",
    "You might have heard about [people who deny climate change.](https://en.wikipedia.org/wiki/Climate_change_denial) How many sceptics are there? Why do they believe so? Let's look at 15000 tweets and analyse people's beliefs on climate change.\n",
    "\n",
    "**Objective**:  \n",
    "\n",
    "Create a model that classifies tweets according to belief in the existence of global warming or climate change. \n",
    "\n",
    "**Dataset**:\n",
    "\n",
    "The labels are the following:\n",
    "- `1` if the tweet suggests anthropogenic (human-induced) climate change is real\n",
    "- `-1` if the tweet is sceptical of anthropogenic climate change\n",
    "- `0` if the tweet is ambiguous, neutral or unrelated to global warming\n",
    "\n",
    "The dataset has been aggregated from the links below. The data obtained from these links has been partially pre-processed so as to present you with an almost balanced multi-class classification problem and to remove non-ASCII characters (and therefore improve the data quality for model training). \n",
    "\n",
    "- https://www.kaggle.com/datasets/edqian/twitter-climate-change-sentiment-dataset\n",
    "- https://data.world/xprizeai-env/sentiment-of-climate-change/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Useful Packages\n",
    "\n",
    "To get started, we will install a number of common machine learning packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn doxa-cli gdown yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import relevant sklearn classes/functions related to data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# For visualising data\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "# For displaying plots on Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "The first step is to download and load the data that we will be using. The data can be downloaded directly from [GitHub](https://github.com/UCLAIS/doxa-challenges/tree/main/Challenge-3) or just by simply running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download the dataset if we don't already have it!\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-3/data/train.csv --output data/train.csv\n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-3/data/test.csv --output data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training dataset\n",
    "data_original = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# We then make a copy of the dataset that we can manipulate\n",
    "# and process while leaving the original intact\n",
    "data = data_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Before we start to train our machine learning model, it is important to have a look at and understand the dataset that we will be using. This will provide some insight into which models, model hyperparameters and loss functions are suitable for the problem we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the shape of our training and testing set\n",
    "print(f\"Shape of the dataset: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the first 15 sample of the dataset\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, from the simple analysis we've done above, we can see that our dataset consists of 15,000 samples (or rather tweets) and our job is to predict the sentiment of these tweets as either -1, 0, or 1.\n",
    "\n",
    "Nice! Now let's try to see whether we are dealing with a balanced classification problem or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4000 data points that correspond to a 'Sentiment' of -1, and 5500 data points that belong to each of 'Sentiment' 0 and 1. The dataset that we have seems a bit imbalanced, but the good thing is that we are not dealing with a heavily imbalanced dataset. This means we can get the ball rolling while not thinking too much about having an imbalanced dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now, we get to one of the unique aspects of dealing with a Natural Language Processing (NLP) problem. As you might know (or might not know), computers can only understand numbers, but when it comes to language, we are dealing with text. A lot of text. This type of data is not really useful for the computer. Thus, it is essential for us to transform the text into something that our machines can understand.\n",
    "\n",
    "And as you might have learned during our tutorial session, we can vectorise our text. So let's vectorise it! We will use the vectors in data visualisation and model training.\n",
    "\n",
    "Before that, let's split our dataset into both training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into input features and target features (labels)\n",
    "data_input = data[\"Tweet\"]\n",
    "data_label = data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    data_input, data_label, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorise our text, we will be using the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) implementation from Scikit-learn. Text preprocessing, tokenisation, and stopword filtering are all included in the CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors.\n",
    "\n",
    "Under the hood, the `CountVectorizer` implementation can be thought of as a transformation to a **bag of words** representation, The brief algorithm is stated below:\n",
    "\n",
    "- Assign a fixed integer ID to each word occurring in any document of the training set (for instance by building a dictionary of integer indicies to corresponding words).\n",
    "- For each document `#i`, count the number of occurrences of each word w and store it in `X[i, j]` as the value of feature #j, where j is the index of word w in the dictionary.\n",
    "- The bag of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising vectorization of climate posts\n",
    "vectoriser = CountVectorizer()\n",
    "X_train_vectorised = vectoriser.fit_transform(X_train)\n",
    "X_valid_vectorised = vectoriser.transform(X_valid)\n",
    "\n",
    "X_train_vectorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorised.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset now has been vectorised, and we can see that it contains approximately 28,000 features, which correspond to the frequencies with which a particular word appears within a piece of text (tweet). \n",
    "\n",
    "This is a lot of features considering that they come from tweets, the lengths of which are fairly short. Is this expected?\n",
    "\n",
    "Well, maybe! There is an incredibly interesting subfield of [internet linguistics](https://gretchenmcculloch.com/book/), which studies how language is used online. In particular, users of social media have over time come to adopt their own slang, use of punctuation and other peculiarities, and users of Twitter are no exception! ðŸ‘€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what the most common words in our dataset are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of the most common words\n",
    "features = vectoriser.get_feature_names_out()\n",
    "visualiser = FreqDistVisualizer(features=features, orient=\"v\")\n",
    "visualiser.fit(X_train_vectorised)\n",
    "visualiser.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, because we are dealing with climate change specific tweets, the words that come up the most include: climate, change, global, warming, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "As you may guess from the name, the [one-vs-the-rest (OvR)](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) approach to building a multi-class model is based on training a number of binary classifiers that try to identify whether or not each instance belongs to any particular class, and then combining the results.\n",
    "\n",
    "For this section, we'll experiment with various models such as a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), a [Bagging Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) using the OvR strategy, and a [Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) Classifier using the OvR strategy. Feel free to try out other models too!\n",
    "\n",
    "If you need a refresher on random forests and gradient boosting, it was covered it in [week 5](https://github.com/UCLAIS/ml-tutorials-season-3/tree/main/week-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Random Forest model\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a bagging OvR classifier\n",
    "bag_classifier = OneVsRestClassifier(BaggingClassifier())\n",
    "bag_classifier.fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gradient Boosting OvR classifier\n",
    "boost_classifier = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "boost_classifier.fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we have trained our machine learning models, we can test them on our validation set.\n",
    "\n",
    "Since we are dealing with a balanced dataset, we will be evaluating using the simple accuracy metric, which is the percentage of correct predictions out of all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .predict() method to predict output values for our test set\n",
    "rf_predicted = rf_classifier.predict(X_valid_vectorised)\n",
    "bag_predicted = bag_classifier.predict(X_valid_vectorised)\n",
    "boost_predicted = boost_classifier.predict(X_valid_vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the accuracy_score() implementation from scikit-learn\n",
    "rf_accuracy = accuracy_score(rf_predicted, y_valid)\n",
    "bag_accuracy = accuracy_score(bag_predicted, y_valid)\n",
    "boost_accuracy = accuracy_score(boost_predicted, y_valid)\n",
    "\n",
    "print(\"Accuracy (Random Forest): \", rf_accuracy)\n",
    "print(\"Accuracy (Bagging Classifier with OvR strategy): \", bag_accuracy)\n",
    "print(\"Accuracy (Boost Classifier with OvR strategy): \", boost_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our Bagging classifier and Gradient Boosting classifier perform worse than our Random Forest classifier for this data. Does this have something to do with the OvR implementation? \n",
    "\n",
    "As practise, try to implement the Bagging Classifier and Gradient Boosting Classifier without the OvR strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our DOXA Submission\n",
    "\n",
    "Once we are confident with the performance of our model, we can start deploying it on the real test dataset for submission to DOXA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import our test dataset and save it in a variable called data_test\n",
    "data_test = pd.read_csv(\"./data/test.csv\")  # Change the path accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we must preprocess the dataset before feeding it into the trained model. Remember that there's only one preprocessing step we've done to our training data, which was to use the `CountVectorizer()` implementation from Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the test set\n",
    "X_test_vectorised = vectoriser.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model did the best, so let's use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on testing set\n",
    "predictions = rf_classifier.predict(X_test_vectorised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the size of our predictions and verify that it matches the size of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"submission\", exist_ok=True)\n",
    "\n",
    "with open(\"submission/y.txt\", \"w\") as f:\n",
    "    f.writelines([f\"{prediction}\\n\" for prediction in predictions])\n",
    "\n",
    "with open(\"submission/doxa.yaml\", \"w\") as f:\n",
    "    f.write(\n",
    "        \"competition: uclais-3\\nenvironment: cpu\\nlanguage: python\\nentrypoint: run.py\"\n",
    "    )\n",
    "\n",
    "with open(\"submission/run.py\", \"w\") as f:\n",
    "    f.write(\"with open('y.txt', 'r') as f: print(f.read().strip())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to DOXA\n",
    "\n",
    "Before you can submit to DOXA, you must first ensure that you are enrolled for the challenge on the DOXA website. Visit [the challenge page](https://doxaai.com/competition/uclais-3) and click \"Enrol\" in the top-right corner.\n",
    "\n",
    "You can then log in using the DOXA CLI by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then submit your results to DOXA by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa upload submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! You have (probably) just uploaded your first submission to DOXA! Take a moment to see where you are on the [scoreboard](https://doxaai.com/competition/uclais-3)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Data Preprocessing**\n",
    "- If you look more closely at the data (tweets) we have, most of them contain the '@' sign followed by a Twitter username. Let's think for a moment &ndash; do we really need this information? Or in a much subtler way, does this information provide any value to our model?\n",
    "- Instead of using the CountVectorizer, why not try using other vectoriser implementations that Scikit-learn provides, such as [Tfidf Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). To be concise, TF-IDF is better than the Count Vectorizer because it not only focuses on the frequency of words present in the corpus, but it also includes the importance of the words. \n",
    "- The labels we are using can be categorised as an ordinal encoding `(-1, 0, 1)`. Would a [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) be more suitable?\n",
    "\n",
    "**2.Deep learning model**\n",
    "- In this notebook, we've only used basic machine learning models that do not rely on neural networks. You might wish to take inspiration from the lecture on RNNs and LSTMs in [week 7](https://github.com/UCLAIS/ml-tutorials-season-3/blob/main/week-7/AI%20tutorial%207.pdf) and try using [Keras's LSTM layers](https://keras.io/api/layers/recurrent_layers/lstm/) to build a powerful deep learning model that might outperform sklearn models. Just be careful that your model does not overfit, since 12,000 data points is not that many!\n",
    "\n",
    "**3.Ensemble Model**  \n",
    "- You can also try an ensemble of different models that can generalise better than a single model.\n",
    "\n",
    "**4. Data Augmentation**  \n",
    "- Our dataset consists of 15,000 tweets for you to play with. Is this enough to generate a model that can understand language? \n",
    "- Given the limited volume of data available, you could consider augmenting the dataset. At its most simple, this could be adding or removing random words. You could also consider creating a new dataset yourself by scraping additional data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
