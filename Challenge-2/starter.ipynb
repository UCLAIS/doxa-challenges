{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCLAIS Tutorial Series Challenge 2\n",
    "\n",
    "We are proud to present you with our second challenge of the 2022-23 UCLAIS tutorial series: CIFAR10 image classification probkem. You will be introduced to a variety of core concepts in Computer Vision and specifically the implementation of Convolutional Neural Network architecture using `TensorFlow`. \n",
    "\n",
    "This Jupyter notebook will guide you through the various general stages involved in end-to-end machine learning projects, including data visualisation, data preprocessing, model selection, model training and model evaluation. Finally, you will get the chance to submit your results to [DOXA](https://doxaai.com/).\n",
    "\n",
    "If you do not already have a DOXA account, you will want to [sign up](https://doxaai.com/sign-up) first before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background & Motivation\n",
    "\n",
    "**CIFAR 10**\n",
    "\n",
    "![title](https://www.researchgate.net/profile/Sanjiv-Kumar-7/publication/221830068/figure/fig1/AS:339906418233347@1458051413482/A-few-example-images-from-the-CIFAR10-dataset-From-top-row-to-bottom-row-the-image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**: Image classification is one of the fundamental tasks in the domain of Computer Vision. It has revolutionized and propelled technological advancements in the most prominent fields, including the automobile industry, healthcare, manufacturing, and more. Hence, for this challenge, our problem would be to predict (or classify) the class of the given image, which comes from the well-known CIFAR-10 dataset. The images in the dataset belongs to 10 different classes.\n",
    "\n",
    "**Objective**: Our objective is to be able to predict the class that each image belong to.\n",
    "\n",
    "**Dataset**: The dataset is based on the following [CIFAR-10 dataset](hhttps://www.cs.toronto.edu/~kriz/cifar.html). We have divided the dataset into **'small dataset'** and **'large dataset'**. The small dataset contains 15,000 images, where each class has 1,500 images. Whereas for the large dataset, it contains 50,000 images in total, where each classs has 5,000 images. The partitioned dataset can be accessed via this [Google Drive](https://drive.google.com/drive/folders/11M8y08hEDTmMpVq3tZCU9ajX7Gui_0nN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Useful Packages\n",
    "\n",
    "To get started, we will install a number of common machine learning packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn doxa-cli gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Import relevant sklearn classes/functions related to data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Import relevant TensorFlow classes\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "      \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The first step is to gather the data that we will be using. The data can be downloaded directly via [Google Drive](https://drive.google.com/drive/folders/11M8y08hEDTmMpVq3tZCU9ajX7Gui_0nN) or just by simply running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download the dataset if we don't already have it!\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    !gdown https://drive.google.com/drive/folders/11M8y08hEDTmMpVq3tZCU9ajX7Gui_0nN -O ./data --folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the small dataset for this tutorial. Feel free to change to the large dataset if you want as it can always improve your model, but at the expense of computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved .npz file\n",
    "data_original = np.load('./data/train_small.npz')\n",
    "data_original = data_original['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the dataset that we can manipulate\n",
    "# and process while leaving the original intact\n",
    "# HINT: Use np.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved label\n",
    "# HINT: You can use np.genfromtxt()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding & Visualisation\n",
    "Before we start to train our Machine Learning model, it is important to have a look and understand first the dataset that we will be using. This will provide some insights onto which model, model hyperparameter, and loss function are suitable for the problem we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the shape of our training and testing set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the label that we will be predicting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the label name\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's have a look on a subset of the images we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a number of images using matplotlib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "For this step, there are two basic things we can do before we start building our Neural Network model\n",
    "\n",
    "**1. Label Encoding**\n",
    "\n",
    "As shown in the previous section, our label is composed of an integer in the range of 0 to 9. This is not really suitable for our neural network and can be improved by using one hot encoding\n",
    "\n",
    "**2. Splitting the Training and Validation Set**\n",
    "\n",
    "The next preprocessing step that need to be done before we can proceed to the training step is to split our dataset into the training set and validation set. The training set will be used for the training of our model while the validation set will be used to compare the performance of different Machine Learning (or Neural Network) models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do One-hot encoding on the label\n",
    "# HINT: Use OneHotEncoder class provided by scikit-learn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our features and output into a training set and a validation set by \n",
    "# HINT: Use train_test_split function from scikit-learn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing CNN\n",
    "\n",
    "Now that we have done all of the required preprocessing steps, we can proceed to the most exciting stage, which is constructing the neural network. For this, we will build a Convolution Neural Network which is a neural network architecture that is well known within the Computer Vision domain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the Neural Network, we will be using the functionality provided by TensorFlow which greatly simplifies the task of building a neural network. This [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers) provides all the building block we can use to construct a Neural Network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    #Feel free to add multiple convolutional layers\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #Flatten\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #Write code for the last output layer\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the summary of the model we've created \n",
    "# HINT: Call .summary() on our model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "This is where the magic happens. We will start training our training set with the neural network architecture that we have created before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the required hyperparameters before starting the training process \n",
    "# HINT: Call .compile()\n",
    "\n",
    "\n",
    "# Start the training and save its progress in a variable called 'history'\n",
    "# HINT: Call .fit() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have trained our model, let's plot how our model performed \n",
    "# on both the training and validation dataset as the number of iteration increases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the observation above, our Neural Network architecture seems to have done a good job since the validation loss keeps getting smaller and smaller in tandem with the training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the Model\n",
    "\n",
    "Let's proceed to analyse our model further. The hope is so that we might be able to capture some insight that can be used to create a better CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the prediction on validation set and set the 'neuron' that has the largest value as our prediction\n",
    "# (remember that we have 10 neurons at the end of our CNN architecture)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing for our true label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix of the true label and predicted label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our DOXA Submission\n",
    "\n",
    "Once we are confident with the performance of our model, we can start deploy our model onto DOXA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission folder by downloading it 'curling' it from Github\n",
    "if not os.path.exists(\"submission\"):\n",
    "  os.makedirs(\"submission\")\n",
    "  !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-2/submission/doxa.yaml --output submission/doxa.yaml\n",
    "  !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-2/submission/run.py --output submission/run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CNN model in the submission folder\n",
    "model.save(\"submission/model\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to DOXA\n",
    "\n",
    "Before you can submit to DOXA, you must first ensure that you are enrolled for the challenge on the DOXA website. Visit [the challenge page](https://doxaai.com/competition/uclais-2) and click \"Enrol\" in the top-right corner.\n",
    "\n",
    "You can then log in using the DOXA CLI by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then submit your results to DOXA by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa upload submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! You have (probably) just uploaded your model to DOXA! Let's give DOXA some time for it to evaluate the performance of your model. You will then be able to see how your model perform on the [scoreboard](https://doxaai.com/competition/uclais-2)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ClimateHack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11841742d11f96be4de93e6e6c2ae1e3a22839abf86213b3c23c4bf0c62307b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
