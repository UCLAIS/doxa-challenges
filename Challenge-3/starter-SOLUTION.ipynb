{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCLAIS Tutorial Series Challenge 3\n",
    "\n",
    "We are proud to present you with our third challenge of the 2022-23 UCLAIS tutorial series: Sentiment Analysis on Climate Change problem. You will be introduced to another super exciting domain in Machine Learning, which is Natural Language Processing ðŸ™€.\n",
    "\n",
    "This Jupyter notebook will guide you through the various general stages involved in end-to-end NLP projects, including data visualisation, data preprocessing, model selection, model training and model evaluation. Finally, you will get the chance to submit your results to [DOXA](https://doxaai.com/).\n",
    "\n",
    "If you do not already have a DOXA account, please [sign up](https://doxaai.com/sign-up) first before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background & Motivation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Background**: \n",
    "\n",
    "You might have heard about [people who deny climate change.](https://en.wikipedia.org/wiki/Climate_change_denial) How many skeptics are there? Why do they believe so? Let's look at 12000 tweets and analyse people's belief on climate change.\n",
    "\n",
    "**Objective**:  \n",
    "\n",
    "Create a model that classify tweets according to belief in the existence of global warming or climate change. \n",
    "\n",
    "**Dataset**:\n",
    "\n",
    "The labels are \"1\" if the tweet suggests global warming is occurring, \"-1\" if the tweet suggests global warming is not occurring, and \"0\" if the tweet is ambiguous or unrelated to global warming.  \n",
    "\n",
    "The dataset is aggregated from the links stated below. The data obtained from these links is processed such that we are dealing with an almost balanced classification problem, and any non-ascii character is removed (just to have a higher quality data). \n",
    "- https://www.kaggle.com/datasets/edqian/twitter-climate-change-sentiment-dataset\n",
    "- https://data.world/xprizeai-env/sentiment-of-climate-change/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Useful Packages\n",
    "\n",
    "To get started, we will install a number of common machine learning packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn doxa-cli gdown yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import relevant sklearn classes/functions related to data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# For visualising data\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "# For displaying plots on Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The first step is to gather the data that we will be using. The data can be downloaded directly via [Google Drive](https://drive.google.com/drive/folders/1xct1L1Cyg1JjGQNDT5fXasEdHsb7sl6I) or just by simply running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download the dataset if we don't already have it!\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-3/data/train.csv --output data/train.csv\n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-3/data/test.csv --output data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training dataset\n",
    "data_original = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# We then make a copy of the dataset that we can manipulate\n",
    "# and process while leaving the original intact\n",
    "data = data_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding \n",
    "Before we start to train our Machine Learning model, it is important to have a look and understand first the dataset that we will be using. This will provide some insights onto which model, model hyperparameter, and loss function are suitable for the problem we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the shape of our training and testing set\n",
    "print(f\"Shape of the dataset: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the first 15 sample of the dataset\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, from the simple analysis we've done above, we can see that our dataset consists of 15,000 sample (or rather tweets) and our job is to predict the sentiment of the tweet given by either -1, 0, or 1.\n",
    "\n",
    "Nice! Now let's try to see whether we are dealing with a balanced classification problem or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4000 data that correspond to a 'Sentiment' of -1, whereas 5500 data belong to both 'Sentiment' 0, and 1. The dataset that we have seems a bit imbalanced, but the good thing is we are not dealing with a heavily imbalanced dataset. So, we could just straightaway get the ball rolling while not thinking too much about having an imbalanced dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now, we get to one of the unique part when dealing with Natural Language Preprocessing (NLP) problem. As you might know (or might not know), computer can only understand in numbers, but when it comes to language, we are dealing with text. A bunch of text. This type of data is not really useful for the computer. Thus, it is essential for us to massage the text into something that our machine can understand.\n",
    "\n",
    "And as you might have learned during our tutorial session, what we can do is to vectorise our text, so let's vectorise it. We will use the vectors in data visualisation and model training.\n",
    "\n",
    "Before that, let's split our dataset into both training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into input features and target features (label)\n",
    "data_input = data[\"Tweet\"]\n",
    "data_label = data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data_input, data_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For vectorizing our text, we will be using [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) implementation from Scikit-learn. Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors.\n",
    "\n",
    "Under the hood, CountVectorizer implementation can be thought of as doing **bag of words** representation, where the brief algorithm is stated below:\n",
    "\n",
    "- Assign a fixed integer id to each word occurring in any document of the training set (for instance by building a dictionary from words to integer indices).\n",
    "- For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j where j is the index of word w in the dictionary.\n",
    "- The bags of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing vectorization of climate posts\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorised = vectorizer.fit_transform(X_train)\n",
    "X_valid_vectorised = vectorizer.transform(X_valid)\n",
    "X_train_vectorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorised.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset now has been vectorised, and we can see that it contains approximately 28,393 features which correspond to the frequency of a particular word occuring in a text (tweet). \n",
    "\n",
    "28,393 features is a lot especially considering that it comes from tweet where the length of the text is usually not that large. Do we expect this? \n",
    "\n",
    "Indeed, bear it mind the dataset we are using is coming from twitter, where it has its own language ðŸ™ƒ. All the different slang, lingo, unscrambled word, and mixed up word. Twitter got all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what is the most common words in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of most common words\n",
    "features = vectorizer.get_feature_names_out()\n",
    "visualizer = FreqDistVisualizer(features=features, orient='v')\n",
    "visualizer.fit(X_train_vectorised)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, because we are dealing with climate change specific tweet, the words that come up the most are something like climate, change, gloval, warming, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "For this section, We'll experiment with various models such as [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Bagging Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) using OvR strategy, and [Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) Classifier using OvR strategy. Feel free to try other models.\n",
    "\n",
    "You're already familiar with Random Forest, and Gradient Boosting as we've covered it in [week 5](https://github.com/UCLAIS/ml-tutorials-season-3/tree/main/week-5).\n",
    "\n",
    "As you could have guessed from its name, [One-vs-the-rest (OvR) Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) outputs a binary prediction, i.e. it predicts if the input belongs to a particular class or not. Rather than trying to learn all the classes, it focuses on one specific class, which might give it advantage in some cases. Given that each classifier fits its own class, OvR requires multiple classifiers in multilabel classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Random Forest model\n",
    "rfClassifier = RandomForestClassifier()\n",
    "rfClassifier.fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a bagging OvR classifier\n",
    "bagClassifier = OneVsRestClassifier(BaggingClassifier())\n",
    "bagClassifier.fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gradient Boosting OvR classifier\n",
    "boostClassifier = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "boostClassifier.fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we have trained our machine learning models, we can test them on our validation set.\n",
    "\n",
    "Since we are dealing with a balanced dataset, we will be evaluating using a simple accuracy metrics, where it simply compare the number of prediction our model got right with the total number of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .predict() method to predict output values for our test set\n",
    "rf_predicted = rfClassifier.predict(X_valid_vectorised)\n",
    "bag_predicted = bagClassifier.predict(X_valid_vectorised)\n",
    "boost_predicted = boostClassifier.predict(X_valid_vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the accuracy_score() implementation from scikit-learn\n",
    "rf_accuracy = accuracy_score(rf_predicted, y_valid)\n",
    "bag_accuracy = accuracy_score(bag_predicted, y_valid)\n",
    "boost_accuracy = accuracy_score(boost_predicted, y_valid)\n",
    "\n",
    "print(\"Accuracy (Random Forest): \", rf_accuracy)\n",
    "print(\"Accuracy (Bagging Classifier with OvR strategy): \", bag_accuracy)\n",
    "print(\"Accuracy (Boost Classifier with OvR strategy): \", boost_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our Bagging classifier and Gradient Boosting classifier is performing worse than Rnadom Forest for this data. Does this has something to do with the OvR implementation? \n",
    "\n",
    "As a practice, try implement Bagging Classifier and Gradient Boosting Classifier without OvR strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our DOXA Submission\n",
    "\n",
    "Once we are confident with the performance of our model, we can start deploying it on the real test dataset for submission to DOXA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import our test dataset and save it in a variable called data_test\n",
    "data_test = pd.read_csv(\"./data/test.csv\")          # Change the path accordingly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we must preprocess the dataset before feeding it into the trained model. Remember before that there's only one preprocessing step we've done to our training data, which is using CountVectorizer() implementation of Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorised the test set\n",
    "X_test_vectorised = vectorizer.transform(X_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our Random Forest model did the best, let's do our inference using Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on testing set\n",
    "predictions = rfClassifier.predict(X_test_vectorised)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the size of our prediction and verify that it satisfies with the amount of test set we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"submission\", exist_ok=True)\n",
    "\n",
    "with open(\"submission/y.txt\", \"w\") as f:\n",
    "    f.writelines([f\"{prediction}\\n\" for prediction in predictions])\n",
    "\n",
    "with open(\"submission/doxa.yaml\", \"w\") as f:\n",
    "    f.write(\"competition: uclais-3\\nenvironment: cpu\\nlanguage: python\\nentrypoint: run.py\")\n",
    "\n",
    "with open(\"submission/run.py\", \"w\") as f:\n",
    "    f.write(\"with open('y.txt', 'r') as f: print(f.read().strip())\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to DOXA\n",
    "\n",
    "Before you can submit to DOXA, you must first ensure that you are enrolled for the challenge on the DOXA website. Visit [the challenge page](https://doxaai.com/competition/uclais-1) and click \"Enrol\" in the top-right corner.\n",
    "\n",
    "You can then log in using the DOXA CLI by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then submit your results to DOXA by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa upload submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! You have (probably) just uploaded your first submission to DOXA! Take a moment to see where you are on the [scoreboard](https://doxaai.com/competition/uclais-3)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Data Preprocessing**\n",
    "- If you look at the data (tweet) we have closer, most of the tweet containt '@' sign followed by a Twitter username. Let's think for a moment, do we really need this information? Or in a much subtle way, does this information provide any value to our model?\n",
    "- Instead of using CountVectorizer, why not try using other type of vectorizer implementation that Scikit learn provided such as [Tfidf Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). To be concise, TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words present in the corpus but also provides the importance of the words. \n",
    "- The label we are using can be categorized as an ordinal encoding (-1, 0, 1). Why not using [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) instead!\n",
    "\n",
    "\n",
    "**2.Deep learning model**\n",
    "- In this notebook, we've only used basic machine learning models that do not rely on neural networks. As you've learned RNN and LSTM in [Week 7](https://github.com/UCLAIS/ml-tutorials-season-3/blob/main/week-7/AI%20tutorial%207.pdf), you can try using [Keras's LSTM layers](https://keras.io/api/layers/recurrent_layers/lstm/) to build a powerful deep learning model that might outperform sklearn models. \n",
    "- Be careful, however, that your model does not overfit as 12000 data points are not that many.\n",
    "\n",
    "**3.Ensemble Model**  \n",
    "- You can also try an ensemble of different models that can generalise better than a single model.\n",
    "\n",
    "**4. Data Augmentation**  \n",
    "- Our dataset consist of 15,000 tweets for you to play with. Is this enough to generate a model that can understand language? \n",
    "- Given a limited number of data to play with, you can consider augmenting the dataset. Think of adding or removing a random word, or even making a new dataset yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "11841742d11f96be4de93e6e6c2ae1e3a22839abf86213b3c23c4bf0c62307b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
