{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCLAIS Tutorial Series Challenge 3\n",
    "\n",
    "We are proud to present you with our third challenge of the 2022-23 UCLAIS tutorial series: Sentiment Analysis on the Climate Change problem. You will be introduced to another super exciting domain in Machine Learning, which is Natural Language Processing ðŸ™€.\n",
    "\n",
    "This Jupyter notebook will guide you through the various general stages involved in end-to-end NLP projects, including data visualisation, data preprocessing, model selection, model training, and model evaluation. Finally, you will get the chance to submit your results to [DOXA](https://doxaai.com/).\n",
    "\n",
    "If you do not already have a DOXA account, please [sign up](https://doxaai.com/sign-up) first before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background & Motivation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Background**: \n",
    "\n",
    "You might have heard about [people who deny climate change.](https://en.wikipedia.org/wiki/Climate_change_denial) How many skeptics are there? Why do they believe so? Let's look at 12000 tweets and analyse people's beliefs on climate change.\n",
    "\n",
    "**Objective**:  \n",
    "\n",
    "Create a model that classifies tweets according to belief in the existence of global warming or climate change. \n",
    "\n",
    "**Dataset**:\n",
    "\n",
    "The labels are \"1\" if the tweet suggests global warming is occurring, \"-1\" if the tweet suggests global warming is not occurring, and \"0\" if the tweet is ambiguous or unrelated to global warming.  \n",
    "\n",
    "The dataset is aggregated from the links stated below. The data obtained from these links is processed such that we are dealing with an almost balanced classification problem, and to remove any non-ascii character (just to have higher quality data). \n",
    "- https://www.kaggle.com/datasets/edqian/twitter-climate-change-sentiment-dataset\n",
    "- https://data.world/xprizeai-env/sentiment-of-climate-change/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Useful Packages\n",
    "\n",
    "To get started, we will install a number of common machine learning packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn doxa-cli gdown yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import relevant sklearn classes/functions related to data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# For visualising data\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "# For displaying plots on Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The first step is to gather the data that we will be using. The data can be downloaded directly via [Google Drive](https://drive.google.com/drive/folders/1xct1L1Cyg1JjGQNDT5fXasEdHsb7sl6I) or just by simply running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download the dataset if we don't already have it!\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-3/data/train.csv --output data/train.csv\n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-3/data/test.csv --output data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training dataset\n",
    "data_original = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# We then make a copy of the dataset that we can manipulate\n",
    "# and process while leaving the original intact\n",
    "data = data_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding \n",
    "Before we start to train our Machine Learning model, it is important to have a look at and understand the dataset that we will be using. This will provide some insight into which models, model hyperparameters, and loss functions are suitable for the problem we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the shape of our training and testing set\n",
    "print(f\"Shape of the dataset: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the first 15 sample of the dataset\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, from the simple analysis we've done above, we can see that our dataset consists of 15,000 samples (or rather tweets) and our job is to predict the sentiment of these tweets as either -1, 0, or 1.\n",
    "\n",
    "Nice! Now let's try to see whether we are dealing with a balanced classification problem or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4000 data points that correspond to a 'Sentiment' of -1, and 5500 data points that belong to each of 'Sentiment' 0 and 1. The dataset that we have seems a bit imbalanced, but the good thing is that we are not dealing with a heavily imbalanced dataset. This means we can get the ball rolling while not thinking too much about having an imbalanced dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now, we get to one of the unique aspects of dealing with a Natural Language Processing (NLP) problem. As you might know (or might not know), computers can only understand numbers, but when it comes to language, we are dealing with text. A lot of text. This type of data is not really useful for the computer. Thus, it is essential for us to transform the text into something that our machines can understand.\n",
    "\n",
    "And as you might have learned during our tutorial session, we can vectorise our text. So let's vectorise it! We will use the vectors in data visualisation and model training.\n",
    "\n",
    "Before that, let's split our dataset into both training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into input features and target features (labels)\n",
    "data_input = data[\"Tweet\"]\n",
    "data_label = data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data_input, data_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorise our text, we will be using the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) implementation from Scikit-learn. Text preprocessing, tokenization, and stopword filtering are all included in the CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors.\n",
    "\n",
    "Under the hood, the CountVectorizer implementation can be thought of as a transformation to a **bag of words** representation, The brief algorithm is stated below:\n",
    "\n",
    "- Assign a fixed integer ID to each word occurring in any document of the training set (for instance by building a dictionary of integer indicies to corresponding words).\n",
    "- For each document #i, count the number of occurrences of each word w and store it in X[i, j] as the value of feature #j, where j is the index of word w in the dictionary.\n",
    "- The bag of words representation implies that n_features is the number of distinct words in the corpus: this number is typically larger than 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing vectorization of climate posts\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorised = vectorizer.fit_transform(X_train)\n",
    "X_valid_vectorised = vectorizer.transform(X_valid)\n",
    "X_train_vectorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorised.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset now has been vectorised, and we can see that it contains approximately 28,000 features which correspond to the frequency of a particular word occuring in a text (tweet). \n",
    "\n",
    "This is a lot of features considering that they come from tweets, where the length of the text is usually not that large. Do we expect this? \n",
    "\n",
    "Indeed, bear in mind the dataset we are using is coming from twitter, which hosts its own language ðŸ™ƒ. All the different slang, lingo, unscrambled words, and mixed up words. Twitter has all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's see what the most common words in our dataset are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of most common words\n",
    "features = vectorizer.get_feature_names_out()\n",
    "visualizer = FreqDistVisualizer(features=features, orient='v')\n",
    "visualizer.fit(X_train_vectorised)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, because we are dealing with climate change specific tweets, the words that come up the most include: climate, change, global, warming, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "For this section, We'll experiment with various models such as a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), a [Bagging Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) using the OvR strategy, and a [Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) Classifier using the OvR strategy. Feel free to try other models too!\n",
    "\n",
    "You're already familiar with Random Forests and Gradient Boosting, as we've covered it in [week 5](https://github.com/UCLAIS/ml-tutorials-season-3/tree/main/week-5).\n",
    "\n",
    "As you could have guessed from its name, [One-vs-the-rest (OvR) Classifiers](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) output a binary prediction. It predicts if the input belongs to a particular class or not. Rather than trying to learn all the classes, it focuses on one specific class, which might give it an advantage in some cases. Given that each classifier fits its own class, OvR requires multiple classifiers for a multilabel classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Random Forest model\n",
    "rfClassifier = RandomForestClassifier()\n",
    "rfClassifier.fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a bagging OvR classifier\n",
    "bagClassifier = OneVsRestClassifier(BaggingClassifier())\n",
    "bagClassifier.fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gradient Boosting OvR classifier\n",
    "boostClassifier = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "boostClassifier.fit(X_train_vectorised, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we have trained our machine learning models, we can test them on our validation set.\n",
    "\n",
    "Since we are dealing with a balanced dataset, we will be evaluating using the simple accuracy metric, which is the percentage of correct predictions out of all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .predict() method to predict output values for our test set\n",
    "rf_predicted = rfClassifier.predict(X_valid_vectorised)\n",
    "bag_predicted = bagClassifier.predict(X_valid_vectorised)\n",
    "boost_predicted = boostClassifier.predict(X_valid_vectorised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the accuracy_score() implementation from scikit-learn\n",
    "rf_accuracy = accuracy_score(rf_predicted, y_valid)\n",
    "bag_accuracy = accuracy_score(bag_predicted, y_valid)\n",
    "boost_accuracy = accuracy_score(boost_predicted, y_valid)\n",
    "\n",
    "print(\"Accuracy (Random Forest): \", rf_accuracy)\n",
    "print(\"Accuracy (Bagging Classifier with OvR strategy): \", bag_accuracy)\n",
    "print(\"Accuracy (Boost Classifier with OvR strategy): \", boost_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that our Bagging classifier and Gradient Boosting classifier perform worse than our Random Forest classifier for this data. Does this have something to do with the OvR implementation? \n",
    "\n",
    "As practise, try to implement the Bagging Classifier and Gradient Boosting Classifier without the OvR strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our DOXA Submission\n",
    "\n",
    "Once we are confident with the performance of our model, we can start deploying it on the real test dataset for submission to DOXA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import our test dataset and save it in a variable called data_test\n",
    "data_test = pd.read_csv(\"./data/test.csv\")          # Change the path accordingly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we must preprocess the dataset before feeding it into the trained model. Remember that there's only one preprocessing step we've done to our training data, which was to use the CountVectorizer() implementation from Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the test set\n",
    "X_test_vectorised = vectorizer.transform(X_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our Random Forest model did the best, let's use it for our inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on testing set\n",
    "predictions = rfClassifier.predict(X_test_vectorised)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the size of our predictions and verify that it matches the size of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"submission\", exist_ok=True)\n",
    "\n",
    "with open(\"submission/y.txt\", \"w\") as f:\n",
    "    f.writelines([f\"{prediction}\\n\" for prediction in predictions])\n",
    "\n",
    "with open(\"submission/doxa.yaml\", \"w\") as f:\n",
    "    f.write(\"competition: uclais-3\\nenvironment: cpu\\nlanguage: python\\nentrypoint: run.py\")\n",
    "\n",
    "with open(\"submission/run.py\", \"w\") as f:\n",
    "    f.write(\"with open('y.txt', 'r') as f: print(f.read().strip())\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to DOXA\n",
    "\n",
    "Before you can submit to DOXA, you must first ensure that you are enrolled for the challenge on the DOXA website. Visit [the challenge page](https://doxaai.com/competition/uclais-3) and click \"Enrol\" in the top-right corner.\n",
    "\n",
    "You can then log in using the DOXA CLI by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then submit your results to DOXA by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa upload submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! You have (probably) just uploaded your first submission to DOXA! Take a moment to see where you are on the [scoreboard](https://doxaai.com/competition/uclais-3)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Data Preprocessing**\n",
    "- If you look more closely at the data (tweets) we have, most of them contain the '@' sign followed by a Twitter username. Let's think for a moment, do we really need this information? Or in a much subtler way, does this information provide any value to our model?\n",
    "- Instead of using the CountVectorizer, why not try using other vectorizer implementations that Scikit-learn provides, such as [Tfidf Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). To be concise, TF-IDF is better than the Count Vectorizer because it not only focuses on the frequency of words present in the corpus, but it also includes the importance of the words. \n",
    "- The labels we are using can be categorized as an ordinal encoding (-1, 0, 1). Why not try using [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) instead!\n",
    "\n",
    "\n",
    "**2.Deep learning model**\n",
    "- In this notebook, we've only used basic machine learning models that do not rely on neural networks. As you've learned RNNs and LSTMs in [Week 7](https://github.com/UCLAIS/ml-tutorials-season-3/blob/main/week-7/AI%20tutorial%207.pdf), you can try using [Keras's LSTM layers](https://keras.io/api/layers/recurrent_layers/lstm/) to build a powerful deep learning model that might outperform sklearn models. \n",
    "- However, be careful that your model does not overfit because 12000 data points isn't that many.\n",
    "\n",
    "**3.Ensemble Model**  \n",
    "- You can also try an ensemble of different models that can generalise better than a single model.\n",
    "\n",
    "**4. Data Augmentation**  \n",
    "- Our dataset consists of 15,000 tweets for you to play with. Is this enough to generate a model that can understand language? \n",
    "- Given a limited volume of data to play with, you can consider augmenting the dataset. Think of adding or removing a random word, or even making a new dataset yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "11841742d11f96be4de93e6e6c2ae1e3a22839abf86213b3c23c4bf0c62307b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
