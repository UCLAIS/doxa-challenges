{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCLAIS Tutorial Series Challenge 2\n",
    "\n",
    "We are proud to present you with the second challenge of the 2022-23 UCLAIS tutorial series: the CIFAR-10 image classification problem. You will be introduced to a variety of core concepts in **computer vision** and specifically the implementation of convolutional neural network (CNN) architectures using the popular machine learning package, [TensorFlow](https://www.tensorflow.org/).\n",
    "\n",
    "This Jupyter notebook will guide you through the various general stages involved in end-to-end machine learning projects, including data visualisation, data preprocessing, model selection, model training and model evaluation. Finally, you will have the opportunity to submit the model you build to [DOXA](https://doxaai.com/) for evaluation on an unseen test set.\n",
    "\n",
    "This notebook contains blank code blocks for you to experiment with your own ideas in! See the `starter-SOLUTION.ipynb` notebook if you need more guidance.\n",
    "\n",
    "If you do not already have a DOXA account, you will want to [sign up](https://doxaai.com/sign-up) first before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background & Motivation\n",
    "\n",
    "**CIFAR 10**\n",
    "\n",
    "![title](./media/CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**: Image classification is one of the fundamental tasks in the domain of computer vision. It has revolutionised and propelled technological advancements in many prominent fields and industries, including healthcare, manufacturing, the automobile industry and much more.\n",
    "\n",
    "**Objective**: For this challenge, your aim is to build a model that can accurately predict the class to which images drawn from the popular CIFAR-10 dataset belong. The images in the dataset can each belong to one of ten different classes.\n",
    "\n",
    "**Dataset**: The dataset is based on the following [CIFAR-10 dataset](hhttps://www.cs.toronto.edu/~kriz/cifar.html). We have divided the dataset into a **'smaller dataset'** (43.9 MB), as well as a **'larger dataset'** (146.5 MB) you can use if you are feeling more comfortable. The small dataset contains 15,000 images, where each class has 1,500 images, whereas the large dataset contains 50,000 images in total, where each class has 5,000 images. In other words, these datasets are _balanced_. The partitioned dataset can be downloaded from [Google Drive](https://drive.google.com/drive/folders/11M8y08hEDTmMpVq3tZCU9ajX7Gui_0nN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Useful Packages\n",
    "\n",
    "To get started, we will install a number of common machine learning packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn doxa-cli gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Import relevant sklearn classes/functions related to data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Import relevant TensorFlow classes\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The first step is to gather the data that we will be using. The data can be downloaded directly via [Google Drive](https://drive.google.com/drive/folders/11M8y08hEDTmMpVq3tZCU9ajX7Gui_0nN) or just by simply running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download the dataset if we don't already have it!\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    !gdown https://drive.google.com/drive/folders/11M8y08hEDTmMpVq3tZCU9ajX7Gui_0nN -O ./data --folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the small dataset in this notebook, but feel absolutely free to switch to the large dataset if you are feeling comfortable. To do so, just comment out the lines that load the features and labels for the small dataset, and then uncomment those corresponding to the large dataset. It will allow you to improve your model further, but you may require additional compute power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select either the \"small\" dataset or the \"large\" dataset\n",
    "DATASET = \"small\"\n",
    "# DATASET = \"large\"\n",
    "\n",
    "# Load the saved .npz file\n",
    "data_original = np.load(f\"./data/train_{DATASET}.npz\")[\"data\"]\n",
    "\n",
    "# Load the labels\n",
    "labels = np.genfromtxt(f\"./data/train_{DATASET}_label.csv\").astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then make an in-memory copy of the dataset that we can manipulate \n",
    "# and experiment with. Just remember to rerun this code block when you\n",
    "# change your data preprocessing approach!\n",
    "\n",
    "data = data_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding & Visualisation\n",
    "Before we start to train our Machine Learning model, it is important to have a look and understand first the dataset that we will be using. This will provide some insights onto which model, model hyperparameter, and loss function are suitable for the problem we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the shape of our training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the labels we will be predicting (integers in the range 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the friendly label names (e.g. `cat`, `dog`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a number of images using matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "For this step, there are two basic things we can do before we start building our neural network model:\n",
    "\n",
    "**1. Label Encoding**\n",
    "\n",
    "As shown in the previous section, our labels are integers in the range of 0 to 9. This is not really suitable for our neural network, so it is recommended instead to one-hot encode the labels using the [LabelBinarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) class provided by Scikit-Learn.\n",
    "\n",
    "**2. Splitting into the Training and Validation Sets**\n",
    "\n",
    "The next preprocessing step that needs to be done before we can proceed onto model training is to split our dataset into the training and validation sets. The training set will be used for the training of our model, while the validation set will be used to evaluate the performance of our model (say, against other models you might train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "\n",
    "# HINT: Use LabelBinarizer class provided by scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our features and output labels into separate training and validation sets\n",
    "\n",
    "# HINT: Use train_test_split function from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Convolutional Neural Network\n",
    "\n",
    "Now that we have completed all of the required preprocessing steps, we can proceed onto the most exciting stage, which is constructing the neural network. For this, we will build a convolutional neural network (CNN), which is a popular network architecture in the domain of computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the neural network, we will be using the functionality provided by TensorFlow, which greatly simplifies the task of building neural networks. The [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers) describes all of the building blocks you can take advantage of to build neural networks in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        # Feel free to add multiple convolutional layers\n",
    "        # Flatten\n",
    "        # Add code for the last output layer\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at a summary of the model we've created\n",
    "\n",
    "# HINT: Call .summary() on our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "This is where the magic happens. We will start training our training set with the neural network architecture that we have created before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the required hyperparameters before starting the training process\n",
    "# HINT: Call .compile()\n",
    "\n",
    "# Start the training and save its progress in a variable called 'history'\n",
    "# HINT: Call .fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have trained our model, let's plot how our model performed\n",
    "# on both the training and validation dataset as the number of iteration increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has your model performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the Model\n",
    "\n",
    "Let's proceed to analyse our model further. The hope is so that we might be able to capture some insight that can be used to improve on our model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run our model on the validation set and base our class predictions on\n",
    "# whichever neuron (of the 10 neurons in the output layer) has the largest value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing for our true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix of the true and predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our DOXA Submission\n",
    "\n",
    "Once we are confident with the performance of our model, we can start deploy our model onto DOXA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission folder by downloading a few required files from Github\n",
    "if not os.path.exists(\"submission\"):\n",
    "    os.makedirs(\"submission\")\n",
    "    \n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-2/submission/doxa.yaml --output submission/doxa.yaml\n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-2/submission/run.py --output submission/run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CNN model in the submission folder\n",
    "model.save(\"submission/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to DOXA\n",
    "\n",
    "Before you can submit to DOXA, you must first ensure that you are enrolled for the challenge on the DOXA website. Visit [the challenge page](https://doxaai.com/competition/uclais-2) and click \"Enrol\" in the top-right corner.\n",
    "\n",
    "You can then log in using the DOXA CLI by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then submit your results to DOXA by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa upload submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! You have (probably) just uploaded your model to DOXA! When DOXA has time to evaluate the performance of your model on an unseen test set, you will then be able to see how your model performs on the [scoreboard](https://doxaai.com/competition/uclais-2)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
