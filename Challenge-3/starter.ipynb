{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCLAIS Tutorial Series Challenge 3\n",
    "\n",
    "We are proud to present you with our third challenge of the 2022-23 UCLAIS tutorial series: Sentiment Analysis on the Climate Change problem. You will be introduced to another super exciting domain in Machine Learning, which is Natural Language Processing ðŸ™€.\n",
    "\n",
    "This Jupyter notebook will guide you through the various general stages involved in end-to-end NLP projects, including data visualisation, data preprocessing, model selection, model training, and model evaluation. Finally, you will get the chance to submit your results to [DOXA](https://doxaai.com/).\n",
    "\n",
    "If you do not already have a DOXA account, please [sign up](https://doxaai.com/sign-up) first before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background & Motivation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Background**: \n",
    "\n",
    "You might have heard about [people who deny climate change.](https://en.wikipedia.org/wiki/Climate_change_denial) How many skeptics are there? Why do they believe so? Let's look at 12000 tweets and analyse people's beliefs on climate change.\n",
    "\n",
    "**Objective**:  \n",
    "\n",
    "Create a model that classifies tweets according to belief in the existence of global warming or climate change. \n",
    "\n",
    "**Dataset**:\n",
    "\n",
    "The labels are \"1\" if the tweet suggests global warming is occurring, \"-1\" if the tweet suggests global warming is not occurring, and \"0\" if the tweet is ambiguous or unrelated to global warming.  \n",
    "\n",
    "The dataset is aggregated from the links stated below. The data obtained from these links is processed such that we are dealing with an almost balanced classification problem, and to remove any non-ascii character (just to have higher quality data). \n",
    "- https://www.kaggle.com/datasets/edqian/twitter-climate-change-sentiment-dataset\n",
    "- https://data.world/xprizeai-env/sentiment-of-climate-change/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Importing Useful Packages\n",
    "\n",
    "To get started, we will install a number of common machine learning packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn doxa-cli gdown yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import relevant sklearn classes/functions related to data preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# For visualising data\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "# For displaying plots on Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The first step is to gather the data that we will be using. The data can be downloaded directly via [Google Drive](https://drive.google.com/drive/folders/1xct1L1Cyg1JjGQNDT5fXasEdHsb7sl6I) or just by simply running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download the dataset if we don't already have it!\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-3/data/train.csv --output data/train.csv\n",
    "    !curl https://raw.githubusercontent.com/UCLAIS/doxa-challenges/main/Challenge-3/data/test.csv --output data/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the training dataset\n",
    "data_original = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# We then make a copy of the dataset that we can manipulate\n",
    "# and process while leaving the original intact\n",
    "data = data_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding \n",
    "Before we start to train our Machine Learning model, it is important to have a look at and understand the dataset that we will be using. This will provide some insight into which models, model hyperparameters, and loss functions are suitable for the problem we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the shape of our training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the first 15 sample of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to see whether we are dealing with an imbalanced or a balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now, we get to one of the unique aspects of dealing with a Natural Language Processing (NLP) problem. As you might know (or might not know), computers can only understand numbers, but when it comes to language, we are dealing with text. A lot of text. This type of data is not really useful for the computer. Thus, it is essential for us to transform the text into something that our machines can understand.\n",
    "\n",
    "And as you might have learned during our tutorial session, we can vectorise our text. So let's vectorise it! We will use the vectors in data visualisation and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into input features and target features (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing vectorization of climate posts using CountVectorizer() or TfifdVectorizer() implementation from Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the distribution of most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try any model here ranging from classical Machine Learning (Gradient Boosting, KNN) to neural network (RNN, Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we have trained our machine learning models, we can test them on our validation set we have created earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .predict() method to predict output values for our test set (if you're using Scikit-learn implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the performance of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our DOXA Submission\n",
    "\n",
    "Once we are confident with the performance of our model, we can start deploying it on the real test dataset for submission to DOXA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import our test dataset and save it in a variable called data_test\n",
    "data_test = pd.read_csv(\"./data/test.csv\")  # Change the path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data preprocessing method you have done in your training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of your prediction, make sure it is the same as the length of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"submission\", exist_ok=True)\n",
    "\n",
    "with open(\"submission/y.txt\", \"w\") as f:\n",
    "    f.writelines([f\"{prediction}\\n\" for prediction in predictions])\n",
    "\n",
    "with open(\"submission/doxa.yaml\", \"w\") as f:\n",
    "    f.write(\n",
    "        \"competition: uclais-3\\nenvironment: cpu\\nlanguage: python\\nentrypoint: run.py\"\n",
    "    )\n",
    "\n",
    "with open(\"submission/run.py\", \"w\") as f:\n",
    "    f.write(\"with open('y.txt', 'r') as f: print(f.read().strip())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to DOXA\n",
    "\n",
    "Before you can submit to DOXA, you must first ensure that you are enrolled for the challenge on the DOXA website. Visit [the challenge page](https://doxaai.com/competition/uclais-3) and click \"Enrol\" in the top-right corner.\n",
    "\n",
    "You can then log in using the DOXA CLI by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then submit your results to DOXA by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!doxa upload submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! You have (probably) just uploaded your first submission to DOXA! Take a moment to see where you are on the [scoreboard](https://doxaai.com/competition/uclais-3)! ðŸ™Œ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "11841742d11f96be4de93e6e6c2ae1e3a22839abf86213b3c23c4bf0c62307b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
